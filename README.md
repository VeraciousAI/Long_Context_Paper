# LLMs for Academic Workflows: An Evaluation of Literature Reviews Generated with Short and Long Context Windows of LLMs


This repository contains the code, data, and results associated with the paper **"Evaluating Large Language Models with Long Context Windows in Academic Literature Reviews."** The study explores how context window size affects the quality, depth, and reliability of AI-generated literature reviews, highlighting key advantages and limitations.

## ðŸ“‚ Repository Structure

- **`data/`** â€“ Contains datasets used for evaluation, sourced from **Semantic Scholar** and **ArXiv**.
- **`code/`** â€“ Includes Python scripts for **data preprocessing, model execution, and evaluation**.
- **`results/`** â€“ Stores AI-generated literature reviews and performance analysis.
- **`figures/`** â€“ Contains visualizations related to evaluation metrics and findings.
- **`README.md`** â€“ This document.

## ðŸš€ Getting Started

### Prerequisites

Ensure you have **Python 3.x** installed, along with the following dependencies:

```bash
pip install numpy pandas scikit-learn matplotlib transformers
