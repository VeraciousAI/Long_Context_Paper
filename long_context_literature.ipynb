{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dHmqgA61xTi"
      },
      "outputs": [],
      "source": [
        "# Install various libraries using pip\n",
        "!pip install google-generativeai tiktoken futures feedparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JPs4AfI4Ncb"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import tiktoken\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO4rxNTP4QTx"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key= userdata.get(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "def gemini(prompt, max_tokens):\n",
        "    # Call gemini 1.5 pro model\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "    response = gemini_model.generate_content(prompt, generation_config=genai.types.GenerationConfig(\n",
        "        max_output_tokens=max_tokens,\n",
        "        temperature=1,\n",
        "    ))\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMAIL = userdata.get(\"EMAIL\")\n",
        "\n",
        "# Call the citeas API for references\n",
        "def get_reference(doi):\n",
        "  url = f\"https://api.citeas.org/product/{doi}?email={EMAIL}\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  data = response.json()\n",
        "  try:\n",
        "    reference = data[\"citations\"][1][\"citation\"]\n",
        "  except:\n",
        "    reference = data[\"citations\"][0][\"citation\"]\n",
        "  reference = reference.replace(\"<i>\", \"\").replace(\"</i>\", \"\")\n",
        "  return reference"
      ],
      "metadata": {
        "id": "kQP5nNCXkVz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-E9Znh24m0k"
      },
      "outputs": [],
      "source": [
        "# Count input tokens\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    tokens = encoding.encode(text)\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JUdThtAHPDu"
      },
      "outputs": [],
      "source": [
        "# Make intext-citations according to number of authors\n",
        "def get_intext_citation(reference):\n",
        "\n",
        "    authors_list = []\n",
        "    pattern = r'^(.*?)\\s*,\\s*(\\d{4})'\n",
        "    match = re.match(pattern, reference)\n",
        "    if match:\n",
        "        authors = match.group(1)\n",
        "        year = match.group(2)\n",
        "\n",
        "    if 'et al.,' in reference:\n",
        "        surname_index = reference.find(',')\n",
        "        surname = reference[:surname_index]\n",
        "        match = re.search(r'\\b\\d{4}\\b', reference)\n",
        "        if match:\n",
        "            year = match.group(0)\n",
        "        intext_citation = f\"{surname} et al. ({year})\"\n",
        "        return intext_citation\n",
        "\n",
        "    elif authors != None:\n",
        "      if '.,' not in authors and '&' in authors:\n",
        "          authors_list = authors.split(\" & \")\n",
        "      elif '.,' not in authors and '&' not in authors:\n",
        "          authors_list = authors.split(\".,\")\n",
        "      else:\n",
        "          authors_list = re.split(r',\\s| & ', authors)\n",
        "\n",
        "    if len(authors_list) == 1:\n",
        "        surname = authors_list[0].split(\",\")\n",
        "        if \" \" in surname:\n",
        "            surname = surname.split(\" \")[1]\n",
        "        intext_citation = f\"{surname[0]} ({year})\"\n",
        "\n",
        "    elif len(authors_list) == 2:\n",
        "        author1_surname = authors_list[0].split(\",\")[0]\n",
        "        if \" \" in author1_surname:\n",
        "            author1_surname = author1_surname.split(\" \")[1]\n",
        "        author2_surname = authors_list[1].split(\",\")[0]\n",
        "        if \" \" in author2_surname:\n",
        "            author2_surname = author2_surname.split(\" \")[1]\n",
        "        intext_citation = f\"{author1_surname} and {author2_surname} ({year})\"\n",
        "\n",
        "    else:\n",
        "        surname = authors_list[0].split(\",\")[0]\n",
        "        intext_citation = f\"{surname} et al. ({year})\"\n",
        "\n",
        "    return intext_citation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDElFnftfQwZ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BaseWebAPIDataLoader(ABC):\n",
        "    def __init__(self, base_url):\n",
        "        self.base_url = base_url\n",
        "\n",
        "    @abstractmethod\n",
        "    def fetch_data(self, search_query, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def make_request(self, endpoint, params=None, headers=None):\n",
        "        url = f\"{self.base_url}{endpoint}\"\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "        print(url)\n",
        "        print(params)\n",
        "        print(headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data\n",
        "        else:\n",
        "            raise Exception(f\"Failed to fetch data from API: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN_qH2cN6FKH"
      },
      "outputs": [],
      "source": [
        "import jellyfish\n",
        "\n",
        "# Get research papers from semantic scholar\n",
        "class SemanticScholarLoader(BaseWebAPIDataLoader):\n",
        "    SS_key = None\n",
        "    def __init__(self,SS_key):\n",
        "        self.SS_key = SS_key\n",
        "        super().__init__(\"https://api.semanticscholar.org/graph/v1/paper/search\")\n",
        "\n",
        "    def fetch_data(self, search_query, limit=12, year_range=None):\n",
        "        headers = {\n",
        "            \"x-api-key\": self.SS_key\n",
        "        }\n",
        "        params = {\n",
        "            \"query\": search_query,\n",
        "            \"limit\": limit,\n",
        "            \"fields\": \"title,url,abstract,authors,citationStyles,journal,year,externalIds\",\n",
        "        }\n",
        "\n",
        "        if year_range is not None:\n",
        "            params[\"year\"] = year_range\n",
        "\n",
        "        data = self.make_request(\"\", params=params, headers=headers)\n",
        "        return data.get(\"data\", [])\n",
        "\n",
        "    def fetch_and_sort_papers(\n",
        "        self,\n",
        "        search_query,\n",
        "        limit=100,\n",
        "        top_n=100,\n",
        "        year_range=None,\n",
        "        weight_similarity=0.7,\n",
        "    ):\n",
        "        papers = []\n",
        "        abstracts = []\n",
        "        references = []\n",
        "        papers.extend(self.fetch_data(research_question, limit, year_range))\n",
        "\n",
        "        for paper in papers:\n",
        "          abstract = paper.get(\"abstract\", \"\")\n",
        "          try:\n",
        "            doi = paper[\"externalIds\"][\"DOI\"]\n",
        "          except:\n",
        "            doi = None\n",
        "\n",
        "          if abstract != None and doi != None:\n",
        "              reference = get_reference(doi)\n",
        "              if \"(n.d.).\" in reference and \"Error: DOI Not Found\" in reference:\n",
        "                continue\n",
        "              else:\n",
        "                intext_citation = get_intext_citation(reference)\n",
        "                references.append(reference)\n",
        "                abstract = f\"{abstract} in-text citations: {intext_citation}\"\n",
        "                abstracts.append(abstract)\n",
        "          else:\n",
        "            continue\n",
        "\n",
        "        return abstracts, references"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "\n",
        "# Get research papers from arxiv\n",
        "def search_arxiv(query):\n",
        "    abstracts = []\n",
        "    references = []\n",
        "    base_url = \"http://export.arxiv.org/api/query?\"\n",
        "\n",
        "    query_params = {\n",
        "        \"search_query\": query,\n",
        "        \"max_results\": 50\n",
        "    }\n",
        "    response = requests.get(base_url, params=query_params)\n",
        "    feed = feedparser.parse(response.content)\n",
        "\n",
        "    for result in feed.entries:\n",
        "        arxiv_id = result.id\n",
        "        abstract = result.summary\n",
        "        if abstract != None and arxiv_id != None:\n",
        "              reference = get_reference(arxiv_id)\n",
        "              if \"(n.d.).\" in reference and \"Error: DOI Not Found\" in reference:\n",
        "                continue\n",
        "              else:\n",
        "                intext_citation = get_intext_citation(reference)\n",
        "                references.append(reference)\n",
        "                abstract = abstract.replace(\"\\n\", \"\")\n",
        "                abstract = f\"{abstract} in-text citations: {intext_citation}\"\n",
        "                abstracts.append(abstract)\n",
        "\n",
        "    return abstracts, references"
      ],
      "metadata": {
        "id": "PC5wY7wGluye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TrXp5ek4_55"
      },
      "outputs": [],
      "source": [
        "# literature review prompt\n",
        "literature_review_prompt = \"\"\"\n",
        "Write a coherent literature review from all provided research papers while addressing the research question \"{research_question}\" for the purpose to help researchers in their research paper.\n",
        "Write professionally in a seamless flow. Must use all the provided research paper in the literature review. Add only one in-text citation per research paper from the attached intext_citations.\n",
        "Write at least a few sentences for every citation and research paper.\n",
        "\n",
        "Research Papers: {abstracts}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt5LF71a4c0_"
      },
      "outputs": [],
      "source": [
        "def write_literature_review(abstracts, research_question):\n",
        "    prompt = literature_review_prompt.format(research_question=research_question, abstracts=abstracts)\n",
        "\n",
        "    # Calculate the tokens in the input\n",
        "    input_tokens = count_tokens(prompt)\n",
        "    print(\"Input tokens:\", input_tokens)\n",
        "\n",
        "    remaining_tokens = 128000 - input_tokens\n",
        "    max_tokens = max(remaining_tokens, 0)\n",
        "\n",
        "    literature_review = gemini(prompt, max_tokens=max_tokens)\n",
        "\n",
        "    return literature_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EScltoz82hRD"
      },
      "outputs": [],
      "source": [
        "def generate_literature_review(research_question, SS_key):\n",
        "    print(f\"Research question: {research_question}\")\n",
        "\n",
        "    abstracts = []\n",
        "    references = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "      arxiv = executor.submit(search_arxiv, research_question)\n",
        "      semantic_scholar = executor.submit(SemanticScholarLoader(SS_key).fetch_and_sort_papers, research_question)\n",
        "\n",
        "      try:\n",
        "        abstracts.extend(arxiv.result()[0])\n",
        "        references.extend(arxiv.result()[1])\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        abstracts.extend(semantic_scholar.result()[0])\n",
        "        references.extend(semantic_scholar.result()[1])\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    if len(abstracts) == 0:\n",
        "        print(\"No papers found for the given research question.\")\n",
        "        return\n",
        "    literature_review_text = write_literature_review(abstracts, research_question)\n",
        "    references_list = \"\\n\".join([f\"{i}. {reference}\" for i, reference in enumerate(references, start=1)])\n",
        "    literature_review_text += f\"\\n\\nReferences: {references_list}\"\n",
        "\n",
        "    print(\"Literature review generated using\", len(abstracts), \"papers.\")\n",
        "    print(\"Literature Review:\", literature_review_text)\n",
        "\n",
        "    return literature_review_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFn-g45i3nOF"
      },
      "outputs": [],
      "source": [
        "# Enter your research question here\n",
        "research_question = input(\"Enter your research question: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "385Y4bWY3lsP"
      },
      "outputs": [],
      "source": [
        "# Semantic scholar API key\n",
        "SS_key = userdata.get(\"SS_key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjQqzlWHWEY2"
      },
      "outputs": [],
      "source": [
        "literature_review = generate_literature_review(research_question, SS_key)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}